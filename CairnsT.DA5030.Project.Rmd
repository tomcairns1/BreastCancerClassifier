---
title: 'Breast Cancer Classifier Using METABRIC Gene Expression Data'
author: Tom Cairns
output: html_document
---

# Business Understanding

Breast Cancer is the second most common type of cancer among women in the US.
Every year over 250,000 women are diagnosed with breast cancer and over 40,000
die from the disease ("Basic Information About Breast Cancer", 2021).

There are many types of breast cancer, but the two most common types are invasive
lobular and ductal carcinoma. These two types of breast cancer have various
genomic differences which distinguish them and respond differently to treatment
(Barroso-Sousa & Metzger-Filho, 2016). Therefore it is essential that these
two types can be distinguished from each other. 

For this project I am particularly interested in classifying these two types of
breast cancer from gene expression data. Gene expression classification could
prove beneficial as it is cheaper than traditional cancer screening methods
(Tobares-Soto et al., 2020). This could be especially useful for developing
countries that do not have the same resources as the US.


# Data Understanding

For this project I am using data acquired by Pereira et al. (2016) in their
attempt to classify cancer types from gene mutations. They sequenced 173 genes
from breast cancer tumor samples of people who had clinical data. They used
unsupervised methods on somatic copy number aberrations with the goal of
idenitfying cancer types. 

One of the important points they make in the paper is that some of the genes
that show high expression might not be from working incorrectly, but rather 
they could be tumor-suppressing genes. This certainly impacts my analysis since
I am classifying on gene expression and there might be some bias introduced by
keeping these tumor-suppressing genes. They could be expressing in one person,
trying to kill the tumor, while suppressed in someone else. The techniques I am
using would not be able to differentiate between "good" expression and "bad"
expression.

My project idea was originally created when I found the METABRIC Breast Cancer
Gene Expression Profiles dataset on kaggle at 
https://www.kaggle.com/datasets/raghadalharbi/breast-cancer-gene-expression-profiles-metabric.

Unfortunately I could not figure out a way to download the data directly from a
URL on kaggle, so I tracked the source of the data to cBioPortal, a repository
of data from genomic cancer studies, and was able to use their API to download
the data directly into this project.

```{r import-libraries, include=FALSE}
# install.packages('BiocManager')
# BiocManager::install('cBioPortalData')
# install.packages('tidyverse')
# install.packages('caret')
# install.packages('class')
# install.packages('gmodels')
# install.packages('vcd')
# install.packages('nnet')
# install.packages('pROC')
# BiocManager::install('smoteFamily')
library(cBioPortalData)
library(tidyverse)
library(caret)
library(class)
library(gmodels)
library(vcd)
library(nnet)
library(pROC)
library(smotefamily)
```

## Explore Data

```{r}
# Import the data
cbio <- cBioPortal()
clinical <- clinicalData(api = cbio, studyId = 'brca_metabric')
all_samples <- allSamples(cbio, 'brca_metabric')
gene_panel <- getGenePanel(cbio, 'METABRIC_173')
gene_data <- molecularData(cbio, molecularProfileIds = 'brca_metabric_mrna', 
                           entrezGeneIds = gene_panel$entrezGeneId, 
                           sampleIds = all_samples$sampleId)
```

In my initial plan for this project I downloaded the data from kaggle:https://www.kaggle.com/datasets/raghadalharbi/breast-cancer-gene-expression-profiles-metabric.
I could not figure out a way to download the data directly from kaggle, but I
was able to find the original source of the data on https://www.cbioportal.org/study/summary?id=brca_metabric.

I used the `cBioPortal()` api to pull clinical data, sample data, and the mrna
expression data from the study.

```{r clean-data}
gene_data <- gene_data$brca_metabric_mrna

# Set breast cancer order
bc_order <- c('Breast Invasive Lobular Carcinoma', 
              'Breast Invasive Ductal Carcinoma')

# Combine data sets and extract important information
bc <- gene_data %>%
  left_join(gene_panel) %>%
  left_join(clinical) %>%
  select(sampleId, value, hugoGeneSymbol, CANCER_TYPE_DETAILED) %>%
  pivot_wider(names_from = hugoGeneSymbol) %>%
  filter(CANCER_TYPE_DETAILED %in% bc_order) %>%
  mutate(cancer_type = factor(CANCER_TYPE_DETAILED, levels = bc_order)) %>%
  select(-CANCER_TYPE_DETAILED)
```

In this section I join the gene_panel and clinical dataframes to the gene_data
data frame. I then select the columns of interest that I will use for this
project (sampleId, patientId, value, hugoGeneSymbol, and cancer_type_detailed).
Since this dataset was in a long format, where each row contained a unique 
sample and gene, I had to pivot it wider so that each row was a unique
sample. Finally I filtered the cancer_type_detailed to contain the two types
of cancer that I am interested in distinguishing between: Breast Invasive
Ductal Carcinoma and Breast Invasive Lobular Carcinoma. I changed this column
to be a factor for ease of use downstream and I changed the name from
CANCER_TYPE_DETAILED to cancer_type.

There are some slight deviations here from my original plan. First, I am
choosing to run a binary classifier for this instead of the three classes that
I had originally planned for. The original data contains 8 classes, but 4 of these
classes have fewer than 30 observations, 1 class contains "mixed" tumors, and
one is "invasive breast carcinoma" which is less descriptive. In my original
plan I had hoped to use the "Breast Mixed Ductal and Lobular Carcinoma" class,
but my models were having a difficult time distinguishing this class from the
other two since it is a combination of both. I decided for simplicity and accuracy
to remove this class from my project.

The other main deviation is in the dimensions of the project. The dataset from
kaggle contained ~476 gene features of data, but pulling this data directly from
the api, I found that it only contains 168 gene features. I am not sure where
the kaggle user obtained data for the other 300 genes, but I am choosing to 
continue with the dataset I have obtained from cbio. It is possible that the
kaggle user pulled other molecular profile Ids, whereas I just pulled from
'brca_metabric_mrna'.

```{r}
# Check missing values
missing_vals <- lapply(bc, is.na)
paste('The number of missing values is:', sum(unlist(missing_vals)))
```

In this code section I check the dataset for missing values. There are no
missing values in the entire data set.

```{r check_normality}
# Function to check normality
isNormal <- function(col) {
    p <- shapiro.test(col)$p.value
    
    # Check if significant
    alpha <- 0.05
    return(p < 0.05)
}

# Find the normality of each column
normal_cols <- lapply(dplyr::select(bc, -sampleId, -cancer_type), isNormal)

# Identify the non normal columns
colnames(bc[,which(normal_cols == F)])

# Create table of target feature
table(bc$cancer_type)
```

In this code block I use a Shapiro-Wilk test of normality to find the columns
that do not express normal distribution. From this I found that genes PIK3CA,
NCOR2, SYNE1, MUC16, and SIK1 are not normally distributed.

I also checked the number of each class in the dataset. The number of instances
of Breast Invasive Lobular Carcinoma is about 1/10th of the instances of
Breast Invasive Ductal Carcinoma. This agrees with the number of diagnoses in
the population as described in the literature (Barroso-Sousa & Metzger-Filho, 2016).
While class imbalance has a major impact on the models, knowing that this is
due to the real-world application and not from poor data collection techniques
means that it is something we need to account for and ensure the model is
robust.

```{r visualize-nonnormal-features}
# Create plots for the not-normally distributed plots
createHist <- function(col) {
  bc %>%
    ggplot(aes(x = col)) +
    geom_histogram(binwidth = 0.1)
}

lapply(bc[,which(normal_cols == F)], createHist)
```

In this code section I created some histograms to visualize the distribution of
the non-normal data. It's apparent that in most cases the data has a bell shape,
but is either slightly skewed or contains outliers. 

```{r normalize-data}
# Normalize the data using z-score
bc.scaled <- as.data.frame(lapply(dplyr::select(bc, -sampleId, -cancer_type), 
                                  scale))

# Function to count number of outliers per column
findOutliers <- function(col) {
  return(sum(abs(col) > 3))
}

# Find these outliers
as.data.frame(lapply(bc.scaled, findOutliers))
```

In this code block I normalized the data by converting the values to z-scores.
I then found the number of outliers for each column, which I am defining as 
greater than 3 standard deviations from the mean. Most columns have a couple
outliers, but CDKN2A, GLDC, TTYH1, COL22A1, and FRMD3 have over 40 outliers
each. I will have to impute the missing values in this data set.

```{r outlier-control}
# Function to replace outliers with NA
removeOutliers <- function(col) {
  return(ifelse(col > 3, median(col), col))
}

# Remove the outliers
bc.removed_outliers <- as.data.frame(lapply(bc.scaled, removeOutliers))
bc.removed_outliers$cancer_type <- bc$cancer_type

# Check the histograms again
lapply(bc.removed_outliers[,which(normal_cols == F)], createHist)
```

In this code block I replaced outliers in each column with the median value
of that column. I chose median because it is less influenced by outliers than
mean. I then recreated the histograms from above and we can see the data appears
much  more normally distributed. 

```{r check-colinearity}
# Create correlation matrix
cor.matrix <- cor(dplyr::select(bc.removed_outliers, -cancer_type))

# Identify columns with potential colinearity
threshold <- 0.9
findCorrelation(cor.matrix, cutoff = threshold)
```

In this section I looked for instances of potential collinearity. I used a threshold
of 0.9 and found no columns with a Pearson's correlation above this value. There
were no columns that showed colinearity. This means I can continue my analysis 
without concern. I used Pearson's correlation coefficient since all of the data
is normally distributed.



# Data Preparation

## Split Data

```{r split-data}
# Set the seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Split the data to maintain proportions of classes
sample <- createDataPartition(y = bc.removed_outliers$cancer_type,
                              p = 0.6,
                              list = F)

# Split into training and testing data
bc_train <- bc.removed_outliers[sample,]
bc_train$cancer_type <- bc.removed_outliers[sample, c('cancer_type')]
remaining <- bc.removed_outliers[-sample,]
remaining$cancer_type <- bc.removed_outliers[-sample, c('cancer_type')]

sample2 <- createDataPartition(y = remaining$cancer_type,
                                  p = 0.5, list = F)

# Split test and validation data
bc_test <- remaining[sample2,]
bc_valid <- remaining[-sample2,]
bc_test$cancer_type <- remaining[sample2, c('cancer_type')]
bc_valid$cancer_type <- remaining[-sample2, c('cancer_type')]


# Check that the data was split properly
cat('\nProportions of training data classes:')
proportions(table(bc_train$cancer_type))
cat('\nProportion of testing data classes:')
proportions(table(bc_test$cancer_type))
cat('\nProportions of validation data classes:')
proportions(table(bc_valid$cancer_type))
```

In this section I split the data into training and validation data sets. I used
the function `createDataPartitions()` to do this after setting the random number
generator and seed. I decided to use a 60/20/20 split for this project where
60% of the data is used for training, 20% is used for testing, and the final
20% is used for validation at the very end.

I then checked the proportions of each class to ensure that there were equivalent
proportions in each data set. As we can see, the function worked correctly and
we do have similar proportions for each of the classes in the training and 
validation data. There is an obvious class imbalance in the data, but this aligns
with real world data where invasive lobular carcinoma makes up about 10% of
overall cases (Barroso-Sousa & Metzger-Filho, 2016).

## Feature Elimination

Feature selection in gene studies, also known as gene selection, is a common
practice (Ang et al., 2016). However, most of the studies that utilize gene
selection have thousands of genes and fewer samples. This leads to the curse
of dimensionality. 

In this project I only have 168 genes and the training data set has close to
1000 observations before oversampling. While there are still many features,
when I ran backward stepwise logistic regression based on AIC (not shown) I was
left with 40 genes of interest. This is a major loss of information and my SVM
and neural network models showed lower accuracy using these selected features.
Therefore, I have decided to remove feature selection from this project and stick
with all 168 genes.

## Oversampling

In this data set I have a major class imbalance problem. This is common in 
gene expression analysis (Ang et al., 2016; Blagus & Lusa, 2012; Mahmudah et al.,
2021; Tabares-Soto et al., 2020). Based on the results by Blagus & Lusa (2012)
and Mahmudah et al., (2021), it seems that the best method of dealing with this 
is by using the Synthetic Minority Oversamplng Technique (SMOTE).

```{r over-sampling}
# Run an oversampling method
minority_cases <- SMOTE(dplyr::select(bc_train, -cancer_type), 
                        target = bc_train$cancer_type, K = 5, dup_size = 0)

# Extract the data and change variable names
minority_cases.data <- minority_cases$data
minority_cases.data$cancer_type = factor(minority_cases.data$class, 
                                         levels = bc_order)
bc_train.oversampled <- dplyr::select(minority_cases.data, -class)

# Check that it worked
table(bc_train.oversampled$cancer_type)
```

In this code block I used the `SMOTE()` function.

I added new instances of the minority class, breast invasive lobular
carcinoma, so that it would be close to the same number of samples as the majority
class, breast invasive ductal carcinoma. 

I created the variable `bc_train.oversampled` which contains near equal sizes of 
each class. The  minority class has 860 instances while the majority has 900
instances.


# Modeling 

## kNN

I decided to use a kNN algorithm for this project since it had been used
previously in the literature (Tabares-Soto et al., 2020). kNN works well for
numeric, normalized data, with a target classification. Therefore it is perfect
for this project. 

```{r find-accuracy-function}
# Function to find the Accuracy
findAccuracy <- function(cm) {
    accuracy = (cm[1] + cm[4]) / sum(cm)
}
```

```{r find-optimal-knn}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Initialize variables
max_k <- 15
accuracy <- rep(0, max_k)
auc_vals <- rep(0, max_k)
kappa_vals <- rep(0, max_k)

# Run the kNN algorithm many times
for (i in 1:max_k) {
  # Run the kNN model
  bc_knn <- knn(train = dplyr::select(bc_train.oversampled, -cancer_type),
                test = dplyr::select(bc_test, -cancer_type),
                cl = as.character(bc_train.oversampled$cancer_type),
                k = i)
  
  # Create CrossTable
  ct <- CrossTable(x = bc_test$cancer_type, 
                   y = bc_knn, prop.chisq = F)$t
  
  # Find Accuracy
  accuracy[i] <- findAccuracy(ct)
  
  # Find AUC
  auc_vals[i] <- auc(as.numeric(factor(bc_test$cancer_type, levels = bc_order)),
                     as.numeric(factor(bc_knn, levels = bc_order)))
  
  # Find Kappa
  kappa_vals[i] <- Kappa(ct)$Unweighted[[1]]
}

# Plot accuracy
data.frame(accuracy) %>%
  ggplot(aes(x = seq(1, max_k), y = accuracy)) +
  geom_line() +
  labs(x = 'k', title = 'kNN Accuracy') +
  theme(axis.line = element_line(), panel.background = element_blank())

# Plot AUC
data.frame(auc_vals) %>%
  ggplot(aes(x = seq(1, max_k), y = auc_vals)) +
  geom_line() +
  labs(x = 'k', title = 'kNN AUC') +
  theme(axis.line = element_line(), panel.background = element_blank())

# Plot Kappa
data.frame(kappa_vals) %>%
  ggplot(aes(x = seq(1, max_k), y = kappa_vals)) +
  geom_line() +
  labs(x = 'k', title = 'kNN Kappa') +
  theme(axis.line = element_line(), panel.background = element_blank())
```

In this section I ran the kNN algorithm 15 times to uncover the model with the
highest accuracy, AUC and Cohen's kappa statistic.

Based on the figures I created from the model, the best value of k to use is 3.
This provides an accuracy around 0.7, an AUC of 0.56, and a kappa statistic of
-0.08. Based on the latter two metrics, this is not the best model to use for
the data since the test data is so imbalanced. 

The AUC appears to follow a trend of decreasing with a higher value of k. The 
kappa seems to be increasing slightly, but all values are within 0.1 of 0,
indicating that a higher value of k would not result in a better kappa statistic.

```{r final-knn}
# Train the model
bc_knn <- knn(train = dplyr::select(bc_train.oversampled, -cancer_type),
                test = dplyr::select(bc_test, -cancer_type), 
                cl = as.character(bc_train.oversampled$cancer_type), k = 3)

# Create the cross table
knn.ct <- CrossTable(x = bc_test$cancer_type, y = bc_knn, prop.chisq = F)

# Calculate the important metrics
knn.acc <- findAccuracy(knn.ct$t)
knn.kappa <- Kappa(knn.ct$t)$Unweighted[[1]]
knn.auc <- auc(as.numeric(factor(bc_knn, levels = bc_order)), 
               as.numeric(factor(bc_test$cancer_type, levels = bc_order)))

# Print the metrics
paste('The accuracy of the model is:', round(knn.acc, 4))
paste('The kappa statistic of the model is:', round(knn.kappa, 4))
paste('The AUC of the model is:', round(knn.auc, 4))
```

After running this algorithm I am not convinced of the effectiveness of this
model. I will keep the code and information here to show why I don't think it's
effective.


## Logistic Regression

Since I have two classes and numeric data, I can run a logistic regression on
the data. I will use AIC to reduce the number of features to find the best model
fit.

```{r logisitic-regression}
# Create simple logisitic regression model
bc_glm <- glm(as.numeric(factor(cancer_type, levels = bc_order)) ~ ., 
              data = bc_train)
summary(bc_glm)

# Use stepwise elimination
bc_glm_step <- step(bc_glm, direction = 'both')
summary(bc_glm_step)

# Find the number of genes
paste('The number of genes involved in the model decreased from',
      length(colnames(bc_train)) - 1, 'to', length(bc_glm_step$coefficients))
```

Here I ran a simple logisitic regression model. I then used both a forward
and backward stepwise algorithm to find a model with the lowest AIC. This 
reduced the number of genes involved in the model from 168 to 51. Of these 51
genes, none appear to have a dominant affect on the outcome, which indicates
that all are involved.

```{r logisitic-regression-evaluation}
# Evaluate the logisitic regression model
bc_glm.pred <- predict(bc_glm_step, bc_test)

# Convert the predictions to categorical values
bc_glm.pred <- ifelse(bc_glm.pred - 1 < 0.5, bc_order[1], bc_order[2])

# Create Cross Table
glm.cross <- CrossTable(x = bc_test$cancer_type, 
                        y = factor(bc_glm.pred, level = bc_order))

# Evaluate
glm.acc <- findAccuracy(glm.cross$t)
paste('The accuracy of the model is:', round(glm.acc, 4))

glm.auc <- auc(as.numeric(factor(bc_test$cancer_type, levels = bc_order)),
               as.numeric(factor(bc_glm.pred, levels = bc_order)))
paste('The AUC of the model is:', round(glm.auc, 4))

glm.kappa <- Kappa(glm.cross$t)
paste('The kappa statistic of the model is:', round(glm.kappa$Unweighted[[1]], 4))
```

This model shows a high accuracy (0.9116), but the AUC and kappa statistic
indicate that the model is no better than just predicting all classes as 
the majority case, breast invasive ductal carcinoma.

This is unfortunate as I hoped this model would show better performance than
the kNN model. 



## SVM

I decided to use a Support Vector Machine supervised learning algorithm for
this project since it was shown by the literature to have strong performance for
classifying cancer type using gene expression (Yuan et al., 2020). SVM is an
algorithm to be used for classification with numeric, normalized data. It works
similarly to kNN except it tries to maximize the distance between two groups.

```{r train-svm}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Create cross validation
control <- trainControl(method = 'cv', number = 10)

# Create the model
bc_svm.l <- train(cancer_type ~ ., data = bc_train.oversampled, 
                method = 'svmLinear3', trControl = control)

# Predict test data
bc_svm.pred_l <- predict(bc_svm.l, bc_test)
```

I created a simple Support Vector Machine model using the svmLinear3 parameter.

```{r evaluate-svm}
# Create CrossTable
svm.cross <- CrossTable(x = bc_test$cancer_type, y = bc_svm.pred_l,
                        prop.chisq = F)

# Find accuracy
accuracy.svm.l <- findAccuracy(svm.cross$t)
paste('The accuracy of the model is:', round(accuracy.svm.l, 4))

# Find AUC
auc.svm.l <- auc(as.numeric(factor(bc_test$cancer_type, levels = bc_order)),
               as.numeric(factor(bc_svm.pred_l, levels = bc_order)))
paste('The AUC of the model is:', round(auc.svm.l, 4))

# Find kappa
kappa.svm.l <- Kappa(svm.cross$t)
paste('The Kappa Statistic is:', round(kappa.svm.l$Unweighted[[1]], 4))
```

Here we can see the accuracy of our model is 80.24% with a kappa statistic of
0.14. This is a much better fit of the data than the kNN model.

```{r hypertune-svm}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Hypertune the model
bc_svm <- train(cancer_type ~ ., data = bc_train.oversampled, 
                method = 'svmRadial', trControl = control)
bc_svm.pred <- predict(bc_svm, bc_test)

# bc_svm.hypertune2 <- train(cancer_type ~ ., data = bc_train.oversampled, 
#                 method = 'svmPoly')
# bc_svm.pred.h2 <- predict(bc_svm.hypertune2, bc_test)

# Create CrossTable
svm.cross <- CrossTable(x = bc_test$cancer_type, y = bc_svm.pred,
                        prop.chisq = F)

# Find accuracy
accuracy.svm <- findAccuracy(svm.cross$t)
paste('The accuracy of the model is:', round(accuracy.svm, 4))

# Find the AUC
auc.svm <- auc(as.numeric(factor(bc_test$cancer_type, levels = bc_order)),
               as.numeric(factor(bc_svm.pred, levels = bc_order)))
paste('The AUC of the model is:', round(auc.svm, 4))

# Find kappa
kappa.svm <- Kappa(svm.cross$t)
paste('The Kappa Statistic is:', round(kappa.svm$Unweighted[[1]], 4))
```

Using the argument `preProcess = c('center', 'scale')` did not improve the model
performance (not pictured). I discovered that using the method 'svmRadial' 
provided a better accuracy and kappa statistic than the linear model and the 
polynomial model. Due to improving the runtime I am not including the output of
the polynomial svm in my final model. 

The accuracy of the radial svm model was 89.67%, the auc was 0.59 and the kappa
statistic was 0.21.

## ANN

I decided to use a simple neural network algorithm for this project since it is
a versatile algorithm. It can be used for classification and numeric, normalized
features. Tabares-Soto et al. (2020) showed the benefit of this algorithm in
predicting cancer type from gene expression data. 

```{r neural-net, message=FALSE}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Find the best number of nodes
max_its <- 10
ann.acc <- rep(0, max_its)
ann.auc <- rep(0, max_its)
ann.k <- rep(0, max_its)

for (i in 1:max_its) {
  # Create the grid of hidden layers
  grid <- expand.grid(size = i, decay = 0)
  
  # Train the model
  ann <- train(cancer_type ~ ., data = bc_train.oversampled, method = 'nnet',
               tunrGrid = grid, trControl = control)
  
  # Find the predictions
  pred <- predict(ann, dplyr::select(bc_test, -cancer_type))
  
  # Create the cross table
  ct <- CrossTable(x = bc_test$cancer_type, y = pred, prop.chisq = F)
  
  # Find accuracy
  ann.acc[i] <- findAccuracy(ct$t)
  
  # Find auc
  ann.auc[i] <- auc(as.numeric(factor(bc_test$cancer_type, levels = bc_order)),
                        as.numeric(factor(pred, levels = bc_order)))
  
  # Find kappa
  ann.k[i] <- Kappa(ct$t)$Unweighted[[1]]
}

# Plot the accuracy
data.frame(ann.acc) %>%
  ggplot(aes(x = seq(1:max_its), y = ann.acc)) +
  geom_line() +
  labs(title = 'Neural Net Accuracy', x = 'Nodes', y = 'Accuracy') +
  theme(panel.background = element_blank(), axis.line = element_line())

# Plot the AUC
data.frame(ann.auc) %>%
  ggplot(aes(x = seq(1:max_its), y = ann.auc)) +
  geom_line() +
  labs(title = 'Neural Net AUC', x = 'Nodes', y = 'AUC') +
  theme(panel.background = element_blank(), axis.line = element_line())

# Plot the kappa statistic
data.frame(ann.k) %>%
  ggplot(aes(x = seq(1:max_its), y = ann.k)) +
  geom_line() + 
  labs(titel = 'Neural Net K', x = 'Nodes', y = 'Kappa') +
  theme(panel.background = element_blank(), axis.line = element_line())
```

Based on these results, my best neural network model contains 3 nodes. This
model has an overall accuracy of almost 88%, an AUC value close to 0.65, and a
kappa statistic of over 0.25. 

```{r evaluate-nn, message=FALSE}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Train the model
grid <- expand.grid(size = 3, decay = 0)
bc_ann <- train(cancer_type ~ ., data = bc_train.oversampled, 
                method = 'nnet', tuneGrid = grid, trControl = control)

# Predict the classes
bc_ann.pred <- predict(bc_ann, bc_test)

# Evaluate the model
ann.ct <- CrossTable(factor(bc_ann.pred, levels = bc_order), 
                     factor(bc_test$cancer_type, levels = bc_order))

# Find the metrics
ann.acc <- findAccuracy(ann.ct$t)
ann.k <- Kappa(ann.ct$t)$Unweighted[[1]]
ann.auc <- auc(as.numeric(factor(bc_ann.pred, levels = bc_order)),
               as.numeric(factor(bc_test$cancer_type, levels = bc_order)))

paste('The accuracy of the final neural net model is:', round(ann.acc, 4))
paste('The kappa of the final neural net model is:', round(ann.k, 4))
paste('The AUC of the final neural net model is:', round(ann.auc, 4))
```

This trained neural network model will be used in my final weighted vote
ensemble below. The accuracy of the model is 84.19%, the kappa statistic is
0.1716, and the AUC is 0.575.

## Ensemble

```{r weighted-vote-ensemble, message=FALSE}
# Set seed and random number generator
RNGversion('3.5.2'); set.seed(123)

# Create a basic ensemble method
runEnsemble <- function(df) {
  
  # Obtain predictions
  knn.en <- knn(train = dplyr::select(bc_train.oversampled, -cancer_type),
              test = dplyr::select(df, -cancer_type),
              cl = as.character(bc_train.oversampled$cancer_type), k = 3)
  svm.en <- predict(bc_svm, df)
  ann.en <- predict(bc_ann, df)
  
  # Find the accuracy values
  knn.acc <- findAccuracy(CrossTable(knn.en, df$cancer_type, prop.chisq = F)$t)
  svm.acc <- findAccuracy(CrossTable(svm.en, df$cancer_type, prop.chisq = F)$t)
  ann.acc<- findAccuracy(CrossTable(ann.en, df$cancer_type, prop.chisq = F)$t)
  
  # Calculate the weights
  total_acc <- sum(knn.acc, svm.acc, ann.acc)
  knn.w <- knn.acc / total_acc
  svm.w <- svm.acc / total_acc
  ann.w <- ann.acc / total_acc

  # Find the predicted class
  predicted_class <- findPrediction(knn.en, svm.en, ann.en, knn.w, svm.w, ann.w)
  
  return(predicted_class)
}

# Function to find the predicted class
findPrediction <- function(knn_m, svm_m, ann_m, knn.w, svm.w, ann.w) {

  # Calculate weighted prediction
  predictions <- (as.numeric(factor(knn_m, levels = bc_order)) - 1) * knn.w + 
    (as.numeric(factor(svm_m, levels = bc_order)) - 1) * svm.w +
    (as.numeric(factor(ann_m, levels = bc_order)) - 1) * ann.w
  
  # Update predictions to be binary
  predictions = ifelse(predictions < 0.5, 'Breast Invasive Lobular Carcinoma', 
                       'Breast Invasive Ductal Carcinoma')

  return(predictions)

}
```

Here I created a bagging ensemble model using a weighted vote. I used the 
accuracy as a weight for the value of each prediction. 

```{r evaluate-ensemble}
final_model <- factor(runEnsemble(bc_valid), levels = bc_order)
final_ct <- CrossTable(final_model, bc_valid$cancer_type)

# Find the accuracy, kappa, and auc
final.acc <- findAccuracy(final_ct$t)
final.k <- Kappa(final_ct$t)$Unweighted[[1]]
final.auc <- auc(as.numeric(final_model), as.numeric(bc_valid$cancer_type))

paste('The accuracy of the final model is:', round(final.acc, 4))
paste('The kappa statistic of the final model is:', round(final.k, 4))
paste('The AUC of the final model is:', round(final.auc, 4))
```

This ensemble model has an overall accuracy of 85.41%, a kappa statistic of
0.308, and an AUC of 0.6281. The accuracy of this model is slightly less than
the accuracy of the radial SVM model on its own, but the kappa statistic and
the AUC of the model are higher, indicating better performance. 


# Conclusions

Overall the accuracy of the model might not be high enough to be used in practice.
I think the largest limitations of this model is the class imbalance of the data.
I would be interested in trying to run the model again, but with more instances
of Breast Invasive Lobular Carcinoma without having to use oversampling.

I was pleased that the accuracy was 85%, which is relatively good at
distinguishing. The kappa statistic and AUC were still relatively low, again
because of the class imbalance problem. 


# References

Ang, J. C., Mirzal, A., Haron, H., & Hamed, H. N. (2016). Supervised, unsupervised, and semi-supervised feature selection: A review on gene selection. *IEEE/ACM Transactions on Computational Biology and Bioinformatics, 13*(5), 971–989. https://doi.org/10.1109/tcbb.2015.2478454 

Barroso-Sousa, R., & Metzger-Filho, O. (2016). Differences between invasive lobular and invasive ductal carcinoma of the breast: Results and therapeutic implications. *Therapeutic Advances in Medical Oncology, 8*(4), 261–266. https://doi.org/10.1177/1758834016644156 

*Basic Information About Breast Cancer*. (2021, September 20). Centers for Disease Control and Prevention. Retrieved April 24, 2022 from, https://www.cdc.gov/cancer/breast/basic_info/index.htm

Blagus, R., & Lusa, L. (2012). Evaluation of smote for high-dimensional class-imbalanced microarray data. *2012 11th International Conference on Machine Learning and Applications*. https://doi.org/10.1109/icmla.2012.183 

Mahmudah, K., Purnama, B., Indriani, F., & Satou, K. (2021). Machine learning algorithms for predicting chronic obstructive pulmonary disease from gene expression data with class imbalance. *Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies*. https://doi.org/10.5220/0010316501480153 

Pereira, B., Chin, S.-F., Rueda, O. M., Vollan, H.-K. M., Provenzano, E., Bardwell, H. A., Pugh, M., Jones, L., Russell, R., Sammut, S.-J., Tsui, D. W., Liu, B., Dawson, S.-J., Abraham, J., Northen, H., Peden, J. F., Mukherjee, A., Turashvili, G., Green, A. R., … Caldas, C. (2016). The somatic mutation profiles of 2,433 breast cancers refine their genomic and transcriptomic landscapes. *Nature Communications, 7*(1). https://doi.org/10.1038/ncomms11479 

Tabares-Soto, R., Orozco-Arias, S., Romero-Cano, V., Segovia Bucheli, V., Rodríguez-Sotelo, J. L., & Jiménez-Varón, C. F. (2020). A comparative study of machine learning and deep learning algorithms to classify cancer types based on microarray gene expression data. *PeerJ Computer Science, 6*. https://doi.org/10.7717/peerj-cs.270 

Yuan, F., Lu, L., & Zou, Q. (2020). Analysis of gene expression profiles of lung cancer subtypes with machine learning algorithms. *Biochimica Et Biophysica Acta (BBA) - Molecular Basis of Disease, 1866*(8), 165822. https://doi.org/10.1016/j.bbadis.2020.165822 

