---
title: 'Final Project'
author: Tom Cairns
output: html_document
---

# Business Understanding


# Data Understanding

```{r}
library(tidyverse)
library(caret)
library(class)
library(gmodels)
library(vcd)
library(nnet)
```

## Explore Data

```{r}
# Import the data
bc_filepath <- 'https://www.kaggle.com/datasets/raghadalharbi/breast-cancer-gene-expression-profiles-metabric/download'
bc <- read_csv(bc_filepath)
dim(bc)
```

```{r select-columns}
# Feature selection
bc <- bc %>%
    select(cancer_type_detailed, !contains('mut'))
bc <- bc[,-c(2:30)]
```

In this code block I selected the features of interest. These include the
cancer_type_detailed, which is our target feature, and the gene expression data.

```{r explore-data}
# Create table of target feature
table(bc$cancer_type_detailed)

# Function to check normality
isNormal <- function(col) {
    p <- shapiro.test(col)$p.value
    
    # Check if significant
    alpha <- 0.05
    return(p < 0.05)
}

# Find the normality of each column
normal_cols <- lapply(bc[,-c(1)], isNormal)

# Identify the non normal columns
which(normal_cols == F)

# Remove non-normal columns
bc <- bc[, -c(which(normal_cols == F))]
```

Here we can see that there were 13 features with non normal distributions. Since
I have many features and the algorithms I will be using assume normality in the
data, I removed these columns.

```{r check-colinearity}
# Create correlation matrix
cor.matrix <- cor(bc[,-c(1)])

# Identify columns with potential colinearity
threshold <- 0.9
findCorrelation(cor.matrix, cutoff = threshold)
```

In this section I looked for instances of potential collinearity. I used a threshold
of 0.9 and found no columns with a Pearson's correlation above this value. This
means I can continue my analysis without concern.


# Data Preparation

```{r remove-classes}
# Remove highly imbalanced classes
bc <- bc[bc$cancer_type_detailed %in% c('Breast Invasive Ductal Carcinoma',
                                        'Breast Invasive Lobular Carcinoma',
                                        'Breast Mixed Ductal and Lobular Carcinoma'),]

table(bc$cancer_type_detailed)
```

In this section I removed three of the total classes from the column 
`cancer_type_detailed`. I removed these classes since they all had fewer than
25 total members. This would create a high bias in the algorithms which might
ignore these classes. There is still some imbalance in the membership of the
classes, but this is much closer to an acceptable split.

## Feature Elimination

```{r feature-elimination, include=FALSE}
# Set the Random Number Generator and seed
RNGversion('3.5.2'); set.seed(5030)

# Convert cancer type to numeric for regression
bc$cancer_type_detailed <- as.numeric(factor(bc$cancer_type_detailed))

# Create initial multiple logisitic regression model
bc.reg <- glm(cancer_type_detailed ~ ., data = bc, family = 'gaussian')
```

```{r}
# Perform the stepwise elimination
bc.step <- step(bc.reg, direction = 'backward')
# beginning AIC was 3996.98, after feature selection it dropped to 3444
```

With 477 features, I was nervous about the curse of dimensionality so I want to
reduce the number of features in my dataset. I accomplished this using backward
stepwise feature elimination using a multiple logistic regression. The model
initially had an AIC of 3999.98, but after removing all but 90 features it
reduced the AIC to 3443.711. 

```{r feature-selection}
# Select the features from the step model
cols.select <- colnames(bc.step$qr$qr)

# Remove the intercept column
cols.select <- cols.select[-1]

# Filter dataset to only include these columns
bc.filter <- bc[, cols.select]
bc.filter$cancer_type_detailed <- bc$cancer_type_detailed
```

In this section I created a new dataframe containing only the features selected
for in the multiple logistic regression model. This reduced the number of features
from 477 to 91.

## Split Data

```{r split-data}
# Split the data to maintain proportions of classes
sample <- createDataPartition(y = bc$cancer_type_detailed,
                              p = 0.7,
                              list = F)

# Split into training and testing data
bc_train <- bc[sample,]
bc_test <- bc[-sample,]

# Check that the data was split properly
cat('\nProportions of training data classes:')
proportions(table(bc_train$cancer_type_detailed))
cat('\nProportion of testing data classes:')
proportions(table(bc_test$cancer_type_detailed))
```

In this section I split the data into training and validation data sets. I used
the function `createDataPartitions()` to do this after setting the random number
generator and seed. I decided to use a 70/30 split for this project. 

I then checked the proportions of each class to ensure that there were equivalent
proportions in each data set. As we can see, the function worked correctly and
we do have similar proportions for each of the classes in the training and 
validation data. 

## Oversampling

In this data set I have a major class imbalance problem. 

# Modeling 

## kNN

```{r}
# Create kNN model
bc.knn <- knn(bc_train[,-c(1)], bc_test[,-c(1)], cl = bc_train$cancer_type_detailed, 
              k = 3)

# Create Cross Table to verify accuracy
knn.cross <- CrossTable(x = bc_test$cancer_type_detailed, y = bc.knn, 
                        prop.chisq = F)
```

In this section I created a k nearest neighbors model using 3 values of k. 
This model has an accuracy of 74.8%. 

The precision of Breast Invasive Ductal Carcinoma is 83.6% (399 / 477) and the
recall is 86.67% (399 / 450).

The precision of Breast Invasic Lobular Carcinoma is 22.2% (8 / 31) and the 
recall is 19% (8 / 42).

The precision of Breast Mixed Ductal and Lobular Carcinoma is 17.4% (8 / 46)
and the recall is 12.9% (8 / 62).

From this model we can see that the most accurate classification is Breast
Invasive Ductal Carcinoma. Neither of the other two categories are well
classified. I think this is largely due to the imbalance in the categories
of the data set.

```{r}
# Function to find the Accuracy
findAccuracy <- function(cm) {
    accuracy = (cm[1] + cm[5] + cm[9]) / sum(cm)
}

# Find the Kappa value
accuracy.knn <- findAccuracy(knn.cross$t)
paste('The accuracy of this kNN model is:', round(accuracy.knn, 3))
kappa.knn <- Kappa(knn.cross$t)
paste('The kappa statistic of the kNN model is:', 
      round(kappa.knn$Unweighted[[1]], 3))
```

This model had an overall accuracy of 74.2% but a Cohen's kappa statistic of
only 0.092. This shows poor performance by the model and reinforces the concern
that the imbalance in the data set is introducing bias in the model.


## SVM

```{r}
# Create the model
bc_svm <- train(cancer_type_detailed ~ ., data = bc_train, method = 'svmLinear3')

# Predict test data
bc_svm.pred <- predict(bc_svm, bc_test)
```

I created a simple Support Vector Machine model using the svmLinear3 parameter.

```{r}
# Create CrossTable
svm.cross <- CrossTable(x = bc_test$cancer_type_detailed, y = bc_svm.pred,
                        prop.chisq = F)

# Find accuracy
accuracy.svm <- findAccuracy(svm.cross$t)
paste('The accuracy of the model is:', round(accuracy.svm, 4))

# Find kappa
kappa.svm <- Kappa(svm.cross$t)
paste('The Kappa Statistic is:', round(kappa.svm$Unweighted[[1]], 4))
```

Here we can see the accuracy of our model is 70%, which is less than the kNN
model. However, the kappa statistic has increased to 0.15.


## ANN

```{r neural-net include=FALSE}
# Train the model
bc_ann <- train(cancer_type_detailed ~ ., data = bc_train, method = 'avNNet', 
                na.action = na.omit)

# Find predictions
bc_ann.pred <- predict(bc_ann, bc_test)
```

```{r evaluate-nn}
# Create CrossTable
ann.cross <- CrossTable(x = bc_test$cancer_type_detailed, y = bc_ann.pred,
                        prop.chisq = F)

# Find accuracy
accuracy.ann <- findAccuracy(ann.cross$t)
paste('The accuracy of the model is:', round(accuracy.ann, 4))

# Find kappa
kappa.ann <- Kappa(ann.cross$t)
paste('The Kappa Statistic is:', round(kappa.ann$Unweighted[[1]], 4))
```

Neural network has classified everything as the Breast Invasive Ductal Carcinoma.

